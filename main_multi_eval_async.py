#!/usr/bin/env python3
# å¼‚æ­¥RAGè¯„ä¼°ç³»ç»Ÿ

import argparse
import asyncio
import sys
import json
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List
from config import CHAT_CONFIG, EMBEDDING_CONFIG, ASYNC_CONFIG, get_enabled_rag_systems, validate_config
from connectors.universal import UniversalRAGConnector
from evaluators.factory import EvaluatorManager

class AsyncMultiEvaluatorRAGSystem:
    """å¼‚æ­¥å¤šè¯„ä¼°å™¨RAGè¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self):
        """ç³»ç»Ÿåˆå§‹åŒ–"""
        # éªŒè¯é…ç½®
        config_errors = validate_config()
        if config_errors:
            raise ValueError(f"é…ç½®é”™è¯¯: {config_errors}")
        
        # åˆå§‹åŒ–RAGè¿æ¥å™¨
        self.connectors = {}
        enabled_systems = get_enabled_rag_systems()
        
        for system_name, config in enabled_systems.items():
            try:
                connector = UniversalRAGConnector(system_name, config)
                self.connectors[system_name] = connector
                print(f"âœ… {system_name} RAGç³»ç»Ÿè¿æ¥å™¨åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                print(f"âŒ {system_name} RAGç³»ç»Ÿåˆå§‹åŒ–é”™è¯¯: {e}")
        
        if not self.connectors:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„RAGç³»ç»Ÿ")
        
        # åˆå§‹åŒ–å¼‚æ­¥è¯„ä¼°å™¨ç®¡ç†å™¨
        self.async_evaluator_manager = EvaluatorManager(CHAT_CONFIG, EMBEDDING_CONFIG)
    
    async def load_test_cases(self, file_path: str) -> list:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise ValueError(f"æµ‹è¯•ç”¨ä¾‹åŠ è½½å¤±è´¥ {file_path}: {e}")
    
    async def test_connections(self) -> Dict[str, bool]:
        """æµ‹è¯•æ‰€æœ‰è¿æ¥"""
        results = {}
        
        for system_name, connector in self.connectors.items():
            try:
                is_connected = await connector.test_connection_async()
                results[system_name] = is_connected
                print(f"{'âœ…' if is_connected else 'âŒ'} {system_name} RAGç³»ç»Ÿè¿æ¥{'æˆåŠŸ' if is_connected else 'å¤±è´¥'}")
            except Exception as e:
                results[system_name] = False
                print(f"âŒ {system_name} RAGç³»ç»Ÿè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    async def query_rag_systems(self, question: str) -> Dict[str, Dict[str, Any]]:
        """æŸ¥è¯¢æ‰€æœ‰RAGç³»ç»Ÿ"""
        results = {}
        
        for system_name, connector in self.connectors.items():
            try:
                result = await connector.query_with_timeout(
                    question, 
                    timeout=ASYNC_CONFIG["rag_query_timeout"]
                )
                results[system_name] = result
                
                if result.get("error"):
                    print(f"  {system_name} é”™è¯¯: {result['error']}")
                else:
                    print(f"  {system_name} æˆåŠŸè·å–å›ç­”")
                    
            except Exception as e:
                results[system_name] = {"answer": "", "contexts": [], "error": str(e)}
                print(f"  {system_name} æŸ¥è¯¢å¤±è´¥: {e}")
        
        return results
    
    async def run_evaluation(self, test_cases: list, connection_results: Dict[str, bool]) -> Dict[str, Any]:
        """è¿è¡Œè¯„ä¼°"""
        evaluation_results = {}
        
        # åªè¯„ä¼°è¿æ¥æˆåŠŸçš„ç³»ç»Ÿ
        successful_systems = [name for name, success in connection_results.items() if success]
        
        if not successful_systems:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„RAGç³»ç»Ÿè¿›è¡Œè¯„ä¼°")
            return evaluation_results
        
        print(f"\nğŸ” å¼€å§‹è¯„ä¼°ï¼Œæµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(test_cases)}")
        
        # å‡†å¤‡è¯„ä¼°æ•°æ®
        all_questions = []
        all_answers = {}
        all_ground_truths = []
        all_contexts = {}
        
        # åˆå§‹åŒ–æ•°æ®ç»“æ„
        for system_name in successful_systems:
            all_answers[system_name] = []
            all_contexts[system_name] = []
        
        # æ”¶é›†æ‰€æœ‰é—®é¢˜å’Œæ ‡å‡†ç­”æ¡ˆ
        for test_case in test_cases:
            question = test_case["question"]
            ground_truth = test_case["ground_truth"]
            
            all_questions.append(question)
            all_ground_truths.append(ground_truth)
        
        # æŸ¥è¯¢æ‰€æœ‰RAGç³»ç»Ÿ
        print("\nğŸ“¡ æŸ¥è¯¢RAGç³»ç»Ÿ...")
        for i, question in enumerate(all_questions):
            print(f"\né—®é¢˜ {i+1}/{len(all_questions)}: {question[:50]}...")
            
            rag_results = await self.query_rag_systems(question)
            
            for system_name in successful_systems:
                result = rag_results.get(system_name, {})
                answer = result.get("answer", "")
                contexts = result.get("contexts", [])
                
                all_answers[system_name].append(answer)
                all_contexts[system_name].append(contexts)
                
                # åœ¨æµ‹è¯•ç”¨ä¾‹ä¸­æ·»åŠ RAGå›ç­”
                if i < len(test_cases):
                    test_cases[i][f"{system_name}_answer"] = answer
                
                print(f"  {system_name} å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
        
        # å¯¹æ¯ä¸ªç³»ç»Ÿè¿›è¡Œè¯„ä¼°
        for system_name in successful_systems:
            print(f"\nğŸ“Š è¯„ä¼° {system_name} ç³»ç»Ÿ...")
            
            try:
                metrics = await self.async_evaluator_manager.evaluate_all_async(
                    all_questions,
                    all_answers[system_name],
                    all_ground_truths,
                    all_contexts[system_name]
                )
                evaluation_results[system_name] = metrics
                print(f"  âœ… {system_name} å¼‚æ­¥è¯„ä¼°å®Œæˆ")
            except Exception as e:
                print(f"  âŒ {system_name} å¼‚æ­¥è¯„ä¼°å¤±è´¥: {e}")
                evaluation_results[system_name] = {}
        
        return evaluation_results
    
    async def save_results(self, evaluation_results: Dict[str, Any], test_cases: list, output_dir: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
        detailed_results = {
            "test_cases": test_cases,
            "evaluation_results": evaluation_results,
            "summary": {}
        }
        
        # è®¡ç®—æ‘˜è¦ç»Ÿè®¡
        for system_name, results in evaluation_results.items():
            detailed_results["summary"][system_name] = {}
            for evaluator_name, metrics in results.items():
                detailed_results["summary"][system_name][evaluator_name] = {}
                for metric_name, values in metrics.items():
                    if values and any(v is not None for v in values):
                        valid_values = [v for v in values if v is not None]
                        detailed_results["summary"][system_name][evaluator_name][metric_name] = {
                            "mean": sum(valid_values) / len(valid_values),
                            "min": min(valid_values),
                            "max": max(valid_values),
                            "count": len(valid_values)
                        }
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_file = output_path / "detailed_evaluation_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆCSVæ ¼å¼ç»“æœ
        csv_data = []
        
        for i, test_case in enumerate(test_cases):
            row = {
                "question": test_case["question"],
                "ground_truth": test_case["ground_truth"]
            }
            
            for system_name, system_results in evaluation_results.items():
                # æ·»åŠ RAGå›ç­”
                if i < len(test_cases):  # å®‰å…¨æ£€æŸ¥
                    row[f"{system_name}_answer"] = test_case.get(f"{system_name}_answer", "")
                
                # æ·»åŠ è¯„ä¼°æŒ‡æ ‡
                for evaluator_name, metrics in system_results.items():
                    for metric_name, values in metrics.items():
                        if i < len(values) and values[i] is not None:
                            row[f"{system_name}_{evaluator_name}_{metric_name}"] = values[i]
                        else:
                            row[f"{system_name}_{evaluator_name}_{metric_name}"] = None
            
            csv_data.append(row)
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_file = output_path / "multi_evaluation_results.csv"
        df = pd.DataFrame(csv_data)
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜:")
        print(f"  è¯¦ç»†ç»“æœ: {json_file}")
        print(f"  CSVç»“æœ: {csv_file}")
    
    async def run(self, test_cases_file: str, output_dir: str):
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¯åŠ¨å¼‚æ­¥å¤šè¯„ä¼°å™¨RAGè¯„ä¼°ç³»ç»Ÿ...")
        
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        test_cases = await self.load_test_cases(test_cases_file)
        print(f"ğŸ“‹ åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        # æµ‹è¯•è¿æ¥
        connection_results = await self.test_connections()
        
        # åˆå§‹åŒ–å¼‚æ­¥è¯„ä¼°å™¨
        await self.async_evaluator_manager.initialize_async()
        
        # è¿è¡Œè¯„ä¼°
        evaluation_results = await self.run_evaluation(test_cases, connection_results)
        
        # ä¿å­˜ç»“æœ
        await self.save_results(evaluation_results, test_cases, output_dir)
        
        print(f"\nğŸ‰ å¼‚æ­¥å¤šè¯„ä¼°å™¨RAGè¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœç›®å½•: {output_dir}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¼‚æ­¥å¤šè¯„ä¼°å™¨RAGè¯„ä¼°ç³»ç»Ÿ")
    parser.add_argument("--test-cases", default="data/test_cases_jp.json", 
                       help="æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/test_cases_jp.json)")
    parser.add_argument("--output", default="results", 
                       help="ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: results)")
    
    args = parser.parse_args()
    
    try:
        evaluator = AsyncMultiEvaluatorRAGSystem()
        await evaluator.run(args.test_cases, args.output)
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())