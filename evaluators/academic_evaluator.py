# å¢å¼ºå­¦æœ¯è¯„ä¼°å™¨ - åˆå¹¶å­¦æœ¯å’Œæ··åˆæ¨¡å‹ä¼˜åŠ¿

from typing import Dict, List, Any, Optional
from langchain_openai import ChatOpenAI
from .base import BaseEvaluator
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.embedding_adapter import EmbeddingAdapterFactory, detect_embedding_config
import json
import re
import asyncio
import aiohttp
import logging

logger = logging.getLogger(__name__)

class AcademicEvaluator(BaseEvaluator):
    """å¢å¼ºå­¦æœ¯è¯„ä¼°å™¨ - æ”¯æŒå¯é€‰çš„åµŒå…¥æ¨¡å‹è¾…åŠ©è¯„ä¼°"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–å¢å¼ºå­¦æœ¯è¯„ä¼°å™¨"""
        super().__init__("Academic", config)
        
        try:
            # åˆå§‹åŒ–èŠå¤©æ¨¡å‹ï¼ˆä¸»è¦è¯„ä¼°æ¨¡å‹ï¼‰
            self.chat_llm = ChatOpenAI(
                api_key=config.get("chat_api_key", config.get("api_key")),
                base_url=config.get("chat_base_url", config.get("base_url")),
                model=config.get("chat_model", config.get("model", "gpt-3.5-turbo")),
                temperature=0
            )
            
            # åˆå§‹åŒ–é€šç”¨åµŒå…¥é€‚é…å™¨
            embedding_config = {
                "api_key": config.get("embedding_api_key", ""),
                "base_url": config.get("embedding_base_url"),
                "model": config.get("embedding_model", "nomic-embed-text:latest"),
                "timeout": config.get("embedding_timeout", 30)
            }
            
            # åˆ›å»ºé€šç”¨åµŒå…¥é€‚é…å™¨
            try:
                self.embedding_adapter = EmbeddingAdapterFactory.create_adapter(embedding_config)
                logger.info(f"âœ… é€šç”¨åµŒå…¥é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ: {embedding_config['model']}")
            except Exception as e:
                logger.warning(f"âš ï¸  åµŒå…¥é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦: {e}")
                self.embedding_adapter = None
            
            # è¯„ä¼°æ¨¡å¼ï¼špure_chatï¼ˆçº¯èŠå¤©æ¨¡å‹ï¼‰æˆ– hybridï¼ˆæ··åˆæ¨¡å¼ï¼‰
            self.evaluation_mode = config.get("evaluation_mode", "pure_chat")
            
            self._available = True
            logger.info(f"âœ… {self.name}å¢å¼ºè¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ (æ¨¡å¼: {self.evaluation_mode})")
        except Exception as e:
            logger.error(f"âŒ {self.name}å¢å¼ºè¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self._available = False
    
    async def evaluate_answers_async(self, questions: List[str], answers: List[str], 
                                   ground_truths: List[str], contexts: List[List[str]] = None) -> Dict[str, List[float]]:
        """å¼‚æ­¥è¯„ä¼°å¤šä¸ªå›ç­”"""
        if not self._available:
            return {"relevancy": [None] * len(answers), 
                   "correctness": [None] * len(answers),
                   "completeness": [None] * len(answers),
                   "clarity": [None] * len(answers)}
        
        # å¹¶å‘è¯„ä¼°æ‰€æœ‰å›ç­”
        tasks = []
        for question, answer, ground_truth in zip(questions, answers, ground_truths):
            if answer and answer.strip():
                task = self.evaluate_single_answer_async(question, answer, ground_truth)
                tasks.append(task)
            else:
                # ä¸ºç©ºå›ç­”åˆ›å»ºé»˜è®¤ç»“æœ
                tasks.append(asyncio.create_task(self._get_default_result()))
        
        # ç­‰å¾…æ‰€æœ‰è¯„ä¼°å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        relevancy_scores = []
        correctness_scores = []
        completeness_scores = []
        clarity_scores = []
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"è¯„ä¼°å¼‚å¸¸: {result}")
                relevancy_scores.append(0.0)
                correctness_scores.append(0.0)
                completeness_scores.append(0.0)
                clarity_scores.append(0.0)
            else:
                relevancy_scores.append(result["relevancy"])
                correctness_scores.append(result["correctness"])
                completeness_scores.append(result["completeness"])
                clarity_scores.append(result["clarity"])
        
        return {
            "relevancy": relevancy_scores,
            "correctness": correctness_scores,
            "completeness": completeness_scores,
            "clarity": clarity_scores
        }
    
    async def evaluate_single_answer_async(self, question: str, answer: str, 
                                         ground_truth: str, context: List[str] = None) -> Dict[str, float]:
        """å¼‚æ­¥è¯„ä¼°å•ä¸ªå›ç­” - æ”¯æŒå¤šç§è¯„ä¼°æ¨¡å¼å’Œè´¨é‡æŒ‡æ ‡"""
        
        try:
            if self.evaluation_mode == "hybrid" and self.embedding_adapter:
                # æ··åˆæ¨¡å¼ï¼šä½¿ç”¨åµŒå…¥æ¨¡å‹è®¡ç®—ç›¸å…³æ€§ï¼ŒèŠå¤©æ¨¡å‹è®¡ç®—è´¨é‡æŒ‡æ ‡
                return await self._evaluate_hybrid_mode(question, answer, ground_truth, context)
            else:
                # çº¯èŠå¤©æ¨¡å¼ï¼šä½¿ç”¨èŠå¤©æ¨¡å‹è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡
                return await self._evaluate_pure_chat_mode(question, answer, ground_truth, context)
                        
        except Exception as e:
            logger.error(f"å¼‚æ­¥è¯„ä¼°é”™è¯¯: {e}")
            return self._get_enhanced_default_scores()
    
    async def _evaluate_hybrid_mode(self, question: str, answer: str, ground_truth: str, context: List[str] = None) -> Dict[str, float]:
        """æ··åˆæ¨¡å¼è¯„ä¼°ï¼šåµŒå…¥æ¨¡å‹ + èŠå¤©æ¨¡å‹"""
        
        try:
            # å¹¶å‘æ‰§è¡Œä¸¤ç§è¯„ä¼°
            relevancy_task = self._calculate_semantic_similarity(answer, ground_truth)
            quality_task = self._assess_enhanced_quality_with_chat_model(question, answer, ground_truth, context)
            
            # ç­‰å¾…ä¸¤ç§è¯„ä¼°å®Œæˆ
            relevancy_score, quality_scores = await asyncio.gather(relevancy_task, quality_task, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸æƒ…å†µ
            if isinstance(relevancy_score, Exception):
                logger.error(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {relevancy_score}")
                relevancy_score = 0.0
            
            if isinstance(quality_scores, Exception):
                logger.error(f"è´¨é‡è¯„ä¼°å¤±è´¥: {quality_scores}")
                quality_scores = self._get_default_quality_scores()
            
            # åˆå¹¶ç»“æœ
            return {
                "relevancy": relevancy_score,
                **quality_scores
            }
            
        except Exception as e:
            logger.error(f"æ··åˆè¯„ä¼°é”™è¯¯: {e}")
            return self._get_enhanced_default_scores()
    
    async def _evaluate_pure_chat_mode(self, question: str, answer: str, ground_truth: str, context: List[str] = None) -> Dict[str, float]:
        """çº¯èŠå¤©æ¨¡å¼è¯„ä¼°ï¼šä½¿ç”¨èŠå¤©æ¨¡å‹è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡"""
        
        enhanced_prompt = f"""
è¯·å¯¹ä»¥ä¸‹å›ç­”è¿›è¡Œå…¨é¢çš„è´¨é‡è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨å¤šä¸ªç»´åº¦ï¼š

**é—®é¢˜**: {question}
**å›ç­”**: {answer}
**æ ‡å‡†ç­”æ¡ˆ**: {ground_truth}
**ä¸Šä¸‹æ–‡**: {context if context else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"}

è¯·ä»ä»¥ä¸‹6ä¸ªç»´åº¦è¯„ä¼°ï¼Œæ¯ä¸ªç»´åº¦ç»™å‡º0.0åˆ°1.0çš„åˆ†æ•°ï¼š

1. **relevancy** (ç›¸å…³æ€§): å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦ï¼Œæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜
2. **correctness** (æ­£ç¡®æ€§): å›ç­”çš„äº‹å®å‡†ç¡®æ€§ï¼Œä¿¡æ¯æ˜¯å¦æ­£ç¡®æ— è¯¯
3. **completeness** (å®Œæ•´æ€§): å›ç­”æ˜¯å¦å…¨é¢ï¼Œæ˜¯å¦æ¶µç›–äº†é‡è¦çš„æ–¹é¢
4. **clarity** (æ¸…æ™°åº¦): å›ç­”çš„è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚ï¼Œé€»è¾‘æ˜¯å¦è¿è´¯
5. **coherence** (è¿è´¯æ€§): å›ç­”çš„ç»“æ„æ˜¯å¦åˆç†ï¼Œæ€è·¯æ˜¯å¦æµç•…
6. **helpfulness** (æœ‰ç”¨æ€§): å›ç­”å¯¹ç”¨æˆ·çš„å®é™…å¸®åŠ©ç¨‹åº¦

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
  "relevancy": åˆ†æ•°,
  "correctness": åˆ†æ•°, 
  "completeness": åˆ†æ•°,
  "clarity": åˆ†æ•°,
  "coherence": åˆ†æ•°,
  "helpfulness": åˆ†æ•°
}}

è¯„ä¼°æ ‡å‡†ï¼š
- 0.9-1.0: ä¼˜ç§€ (Excellent)
- 0.8-0.9: è‰¯å¥½ (Good) 
- 0.6-0.8: ä¸€èˆ¬ (Fair)
- 0.4-0.6: è¾ƒå·® (Poor)
- 0.0-0.4: å¾ˆå·® (Very Poor)
"""
        
        try:
            # ä½¿ç”¨å¼‚æ­¥HTTPè¯·æ±‚
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout)) as session:
                headers = {
                    "Authorization": f"Bearer {self.config.get('chat_api_key', self.config.get('api_key'))}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "model": self.config.get("chat_model", self.config.get("model", "gpt-3.5-turbo")),
                    "messages": [
                        {"role": "user", "content": enhanced_prompt}
                    ],
                    "temperature": 0
                }
                
                logger.debug(f"ğŸ” å¢å¼ºå¼‚æ­¥è¯„ä¼°è¯·æ±‚å‘é€ä¸­...")
                async with session.post(
                    f"{self.config.get('chat_base_url', self.config.get('base_url')).rstrip('/')}/chat/completions",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        result_text = result["choices"][0]["message"]["content"].strip()
                        logger.debug(f"ğŸ” å¢å¼ºå¼‚æ­¥è¯„ä¼°å“åº”æ¥æ”¶: {result_text[:100]}...")
                        
                        # è§£æå¢å¼ºè¯„åˆ†
                        return self._parse_enhanced_scores(result_text)
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                        return self._get_enhanced_default_scores()
                        
        except Exception as e:
            logger.error(f"çº¯èŠå¤©æ¨¡å¼è¯„ä¼°é”™è¯¯: {e}")
            return self._get_enhanced_default_scores()
    
    async def _calculate_semantic_similarity(self, answer: str, ground_truth: str) -> float:
        """ä½¿ç”¨åµŒå…¥æ¨¡å‹è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæ··åˆæ¨¡å¼ç”¨ï¼‰- ä½¿ç”¨é€šç”¨é€‚é…å™¨"""
        
        try:
            # å¦‚æœæ²¡æœ‰åµŒå…¥é€‚é…å™¨ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦
            if not self.embedding_adapter:
                logger.debug("ğŸ” åµŒå…¥é€‚é…å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦")
                return self._calculate_text_similarity(answer, ground_truth)
            
            # å¹¶å‘è·å–ä¸¤ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡
            answer_task = self.embedding_adapter.embed_query(answer)
            ground_truth_task = self.embedding_adapter.embed_query(ground_truth)
            
            answer_embedding, ground_truth_embedding = await asyncio.gather(
                answer_task, ground_truth_task, return_exceptions=True
            )
            
            # å¤„ç†å¼‚å¸¸æƒ…å†µ
            if isinstance(answer_embedding, Exception):
                logger.error(f"âŒ å›ç­”åµŒå…¥å‘é‡è·å–å¤±è´¥: {answer_embedding}")
                return self._calculate_text_similarity(answer, ground_truth)
            
            if isinstance(ground_truth_embedding, Exception):
                logger.error(f"âŒ æ ‡å‡†ç­”æ¡ˆåµŒå…¥å‘é‡è·å–å¤±è´¥: {ground_truth_embedding}")
                return self._calculate_text_similarity(answer, ground_truth)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            if len(answer_embedding) > 0 and len(ground_truth_embedding) > 0:
                similarity = self._calculate_cosine_similarity(answer_embedding, ground_truth_embedding)
                logger.debug(f"ğŸ” åµŒå…¥å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦: {similarity:.4f}")
                return similarity
            else:
                logger.error(f"âŒ åµŒå…¥å‘é‡ä¸ºç©º - answer: {len(answer_embedding)}, ground_truth: {len(ground_truth_embedding)}")
                return self._calculate_text_similarity(answer, ground_truth)
                        
        except Exception as e:
            logger.error(f"åµŒå…¥æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return self._calculate_text_similarity(answer, ground_truth)
    
    async def _assess_enhanced_quality_with_chat_model(self, question: str, answer: str, ground_truth: str, context: List[str] = None) -> Dict[str, float]:
        """ä½¿ç”¨èŠå¤©æ¨¡å‹è¿›è¡Œå¢å¼ºè´¨é‡è¯„ä¼°ï¼ˆæ··åˆæ¨¡å¼ç”¨ï¼‰"""
        
        quality_prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ï¼Œé‡ç‚¹å…³æ³¨æ­£ç¡®æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ‰ç”¨æ€§ï¼š

é—®é¢˜: {question}
å›ç­”: {answer}
æ ‡å‡†ç­”æ¡ˆ: {ground_truth}
ä¸Šä¸‹æ–‡: {context if context else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"}

è¯·ä»ä»¥ä¸‹5ä¸ªç»´åº¦è¯„ä¼°ï¼Œæ¯ä¸ªç»´åº¦ç»™å‡º0.0åˆ°1.0çš„åˆ†æ•°ï¼š

1. **correctness**: äº‹å®å‡†ç¡®æ€§ï¼Œä¸æ ‡å‡†ç­”æ¡ˆçš„ä¸€è‡´ç¨‹åº¦
2. **completeness**: æ˜¯å¦æ¶µç›–äº†æ ‡å‡†ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯
3. **clarity**: è¡¨è¾¾æ˜¯å¦æ¸…æ™°ã€é€»è¾‘æ˜¯å¦è¿è´¯
4. **coherence**: ç»“æ„æ˜¯å¦åˆç†ï¼Œæ€è·¯æ˜¯å¦æµç•…
5. **helpfulness**: å¯¹ç”¨æˆ·çš„å®é™…å¸®åŠ©ç¨‹åº¦

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
  "correctness": åˆ†æ•°,
  "completeness": åˆ†æ•°,
  "clarity": åˆ†æ•°,
  "coherence": åˆ†æ•°,
  "helpfulness": åˆ†æ•°
}}
"""
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=45)) as session:
                headers = {
                    "Authorization": f"Bearer {self.config.get('chat_api_key', self.config.get('api_key'))}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "model": self.config.get("chat_model", self.config.get("model", "gpt-3.5-turbo")),
                    "messages": [
                        {"role": "user", "content": quality_prompt}
                    ],
                    "temperature": 0
                }
                
                async with session.post(
                    f"{self.config.get('chat_base_url', self.config.get('base_url')).rstrip('/')}/chat/completions",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        result_text = result["choices"][0]["message"]["content"].strip()
                        logger.debug(f"ğŸ” è´¨é‡è¯„ä¼°å“åº”: {result_text[:100]}...")
                        
                        return self._parse_quality_scores(result_text)
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ èŠå¤©æ¨¡å‹è¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                        return self._get_default_quality_scores()
                        
        except Exception as e:
            logger.error(f"èŠå¤©æ¨¡å‹è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return self._get_default_quality_scores()
    
    def _parse_scores(self, result_text: str) -> Dict[str, float]:
        """è§£æåŸºæœ¬è¯„åˆ†ç»“æœï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self._parse_enhanced_scores(result_text)
    
    def _parse_enhanced_scores(self, result_text: str) -> Dict[str, float]:
        """è§£æå¢å¼ºè¯„åˆ†ç»“æœ"""
        scores = {
            "relevancy": 0.0, "correctness": 0.0, "completeness": 0.0, 
            "clarity": 0.0, "coherence": 0.0, "helpfulness": 0.0
        }
        
        # æ–¹æ³•1: JSONå½¢å¼ã‚’æ¢ã™
        json_match = re.search(r'\{[^}]*"relevancy"[^}]*\}', result_text, re.DOTALL)
        if json_match:
            try:
                scores_dict = json.loads(json_match.group())
                for metric in ["relevancy", "correctness", "completeness", "clarity", "coherence", "helpfulness"]:
                    score = scores_dict.get(metric, 0.0)
                    scores[metric] = max(0.0, min(1.0, float(score)))
                return scores
            except:
                pass
        
        # æ–¹æ³•2: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
        patterns = {
            "relevancy": r"relevancy[^0-9]*([0-9.]+)",
            "correctness": r"correctness[^0-9]*([0-9.]+)", 
            "completeness": r"completeness[^0-9]*([0-9.]+)",
            "clarity": r"clarity[^0-9]*([0-9.]+)",
            "coherence": r"coherence[^0-9]*([0-9.]+)",
            "helpfulness": r"helpfulness[^0-9]*([0-9.]+)"
        }
        
        for metric, pattern in patterns.items():
            match = re.search(pattern, result_text, re.IGNORECASE)
            if match:
                try:
                    scores[metric] = max(0.0, min(1.0, float(match.group(1))))
                except:
                    scores[metric] = 0.5
        
        logger.debug(f"è§£æçš„å¢å¼ºè¯„åˆ†: {scores}")
        return scores
    
    def _parse_quality_scores(self, result_text: str) -> Dict[str, float]:
        """è§£æè´¨é‡è¯„åˆ†ç»“æœï¼ˆæ··åˆæ¨¡å¼ç”¨ï¼‰"""
        scores = {"correctness": 0.0, "completeness": 0.0, "clarity": 0.0, "coherence": 0.0, "helpfulness": 0.0}
        
        # æ–¹æ³•1: JSONå½¢å¼ã‚’æ¢ã™
        json_match = re.search(r'\{[^}]*"correctness"[^}]*\}', result_text, re.DOTALL)
        if json_match:
            try:
                scores_dict = json.loads(json_match.group())
                for metric in ["correctness", "completeness", "clarity", "coherence", "helpfulness"]:
                    score = scores_dict.get(metric, 0.0)
                    scores[metric] = max(0.0, min(1.0, float(score)))
                return scores
            except:
                pass
        
        # æ–¹æ³•2: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
        patterns = {
            "correctness": r"correctness[^0-9]*([0-9.]+)",
            "completeness": r"completeness[^0-9]*([0-9.]+)",
            "clarity": r"clarity[^0-9]*([0-9.]+)",
            "coherence": r"coherence[^0-9]*([0-9.]+)",
            "helpfulness": r"helpfulness[^0-9]*([0-9.]+)"
        }
        
        for metric, pattern in patterns.items():
            match = re.search(pattern, result_text, re.IGNORECASE)
            if match:
                try:
                    scores[metric] = max(0.0, min(1.0, float(match.group(1))))
                except:
                    scores[metric] = 0.5
        
        logger.debug(f"è§£æçš„è´¨é‡è¯„åˆ†: {scores}")
        return scores
    
    async def _get_default_result(self) -> Dict[str, float]:
        """è·å–é»˜è®¤ç»“æœï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self._get_enhanced_default_scores()
    
    def _get_enhanced_default_scores(self) -> Dict[str, float]:
        """è·å–å¢å¼ºé»˜è®¤è¯„åˆ†"""
        return {
            "relevancy": 0.0, "correctness": 0.0, "completeness": 0.0, 
            "clarity": 0.0, "coherence": 0.0, "helpfulness": 0.0
        }
    
    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            import math
            
            if len(vec1) != len(vec2):
                logger.error(f"âŒ å‘é‡ç»´åº¦ä¸åŒ¹é…: {len(vec1)} vs {len(vec2)}")
                return 0.0
            
            # è®¡ç®—ç‚¹ç§¯
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            
            # è®¡ç®—å‘é‡é•¿åº¦
            magnitude1 = math.sqrt(sum(a * a for a in vec1))
            magnitude2 = math.sqrt(sum(b * b for b in vec2))
            
            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            cosine_similarity = dot_product / (magnitude1 * magnitude2)
            
            # ç¡®ä¿ç»“æœåœ¨[0, 1]èŒƒå›´å†…
            return max(0.0, min(1.0, cosine_similarity))
            
        except Exception as e:
            logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆå¤‡é€‰æ–¹æ³•ï¼Œå½“embeddingä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        try:
            # ç®€å•çš„è¯æ±‡é‡å ç›¸ä¼¼åº¦è®¡ç®—
            import re
            from collections import Counter
            
            # åˆ†è¯
            words1 = set(re.findall(r'\b\w+\b', text1.lower()))
            words2 = set(re.findall(r'\b\w+\b', text2.lower()))
            
            if not words1 or not words2:
                return 0.0
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = words1.intersection(words2)
            union = words1.union(words2)
            
            similarity = len(intersection) / len(union) if union else 0.0
            return similarity
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _get_default_quality_scores(self) -> Dict[str, float]:
        """è·å–é»˜è®¤è´¨é‡è¯„åˆ†"""
        return {"correctness": 0.0, "completeness": 0.0, "clarity": 0.0, "coherence": 0.0, "helpfulness": 0.0}
    
    def get_supported_metrics(self) -> List[str]:
        """è·å–æ”¯æŒçš„è¯„ä»·æŒ‡æ ‡"""
        # æ ¹æ®è¯„ä¼°æ¨¡å¼è¿”å›ä¸åŒçš„æŒ‡æ ‡
        if self.evaluation_mode == "hybrid":
            return ["relevancy", "correctness", "completeness", "clarity", "coherence", "helpfulness"]
        else:
            return ["relevancy", "correctness", "completeness", "clarity", "coherence", "helpfulness"]
    
    # ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™åŒæ­¥æ–¹æ³•
    def evaluate_answers(self, questions: List[str], answers: List[str], 
                        ground_truths: List[str], contexts: List[List[str]] = None) -> Dict[str, List[float]]:
        """åŒæ­¥è¯„ä¼°æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰"""
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.evaluate_answers_async(questions, answers, ground_truths, contexts)
        )
    
    def evaluate_single_answer(self, question: str, answer: str, ground_truth: str) -> Dict[str, float]:
        """åŒæ­¥å•ä¸ªè¯„ä¼°æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰"""
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.evaluate_single_answer_async(question, answer, ground_truth)
        )