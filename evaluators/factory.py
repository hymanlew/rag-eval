# è¯„ä¼°å™¨å·¥å‚ - è¯„ä¼°å™¨çš„åˆ›å»ºå’Œç®¡ç†

from typing import Dict, List, Any, Optional
from .base import BaseEvaluator
from .academic_evaluator import AcademicEvaluator
from .ragas_evaluator import RagasEvaluator
import asyncio
import logging

logger = logging.getLogger(__name__)

class EvaluatorFactory:
    """è¯„ä¼°å™¨å·¥å‚ç±»"""
    
    # å¯ç”¨çš„è¯„ä¼°å™¨ç±»å‹
    EVALUATOR_TYPES = {
        "academic": AcademicEvaluator,
        "ragas": RagasEvaluator
    }
    
    # é»˜è®¤è¯„ä¼°å™¨ä¼˜å…ˆçº§
    DEFAULT_PRIORITY = ["ragas", "academic"]
    
    @classmethod
    async def create_evaluator_async(cls, evaluator_type: str, config: Dict[str, Any]) -> Optional[BaseEvaluator]:
        """å¼‚æ­¥åˆ›å»ºè¯„ä¼°å™¨"""
        if evaluator_type not in cls.EVALUATOR_TYPES:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯„ä¼°å™¨ç±»å‹: {evaluator_type}")
        
        evaluator_class = cls.EVALUATOR_TYPES[evaluator_type]
        
        try:
            evaluator = evaluator_class(config)
            if evaluator.is_available():
                return evaluator
            else:
                logger.warning(f"âš ï¸  {evaluator_type}è¯„ä¼°å™¨ä¸å¯ç”¨")
                return None
        except Exception as e:
            logger.error(f"âŒ {evaluator_type}è¯„ä¼°å™¨åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    @classmethod
    async def create_all_evaluators_async(cls, config: Dict[str, Any], 
                                        types: Optional[List[str]] = None) -> Dict[str, BaseEvaluator]:
        """å¼‚æ­¥åˆ›å»ºæ‰€æœ‰å¯ç”¨çš„è¯„ä¼°å™¨"""
        if types is None:
            types = cls.DEFAULT_PRIORITY
        
        evaluators = {}
        
        # å¹¶å‘åˆ›å»ºæ‰€æœ‰è¯„ä¼°å™¨
        tasks = []
        for evaluator_type in types:
            task = cls.create_evaluator_async(evaluator_type, config)
            tasks.append((evaluator_type, task))
        
        # ç­‰å¾…æ‰€æœ‰è¯„ä¼°å™¨åˆ›å»ºå®Œæˆ
        for evaluator_type, task in tasks:
            evaluator = await task
            if evaluator:
                evaluators[evaluator_type] = evaluator
        
        return evaluators
    
    @classmethod
    def get_evaluator_info(cls) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰è¯„ä¼°å™¨ä¿¡æ¯"""
        info = {}
        
        for name, evaluator_class in cls.EVALUATOR_TYPES.items():
            # ä½¿ç”¨ä¸´æ—¶é…ç½®è·å–ä¿¡æ¯
            dummy_config = {
                "api_key": "dummy",
                "base_url": "dummy",
                "model": "dummy",
                "timeout": 30
            }
            
            try:
                temp_evaluator = evaluator_class(dummy_config)
                info[name] = {
                    "name": temp_evaluator.name,
                    "supported_metrics": temp_evaluator.get_supported_metrics(),
                    "description": cls._get_evaluator_description(name)
                }
            except:
                info[name] = {
                    "name": name,
                    "supported_metrics": [],
                    "description": cls._get_evaluator_description(name)
                }
        
        return info
    
    @classmethod
    def _get_evaluator_description(cls, evaluator_type: str) -> str:
        """è·å–è¯„ä¼°å™¨æè¿°"""
        descriptions = {
            "academic": "å¢å¼ºå­¦æœ¯è¯„ä¼°å™¨ - æ”¯æŒ6ç»´åº¦è´¨é‡è¯„ä¼°ï¼ˆç›¸å…³æ€§ã€æ­£ç¡®æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€è¿è´¯æ€§ã€æœ‰ç”¨æ€§ï¼‰",
            "ragas": "Ragasæ¡†æ¶è¯„ä¼°å™¨ - å®Œæ•´çš„RAGè¯„ä¼°æŒ‡æ ‡é›†ï¼ˆç›¸å…³æ€§ã€æ­£ç¡®æ€§ã€å¿ å®æ€§ã€ä¸Šä¸‹æ–‡ç²¾åº¦ã€ä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰"
        }
        return descriptions.get(evaluator_type, "æ— æè¿°")

class EvaluatorManager:
    """è¯„ä¼°å™¨ç®¡ç†å™¨"""
    
    def __init__(self, chat_config: Dict[str, Any], embedding_config: Dict[str, Any]):
        """åˆå§‹åŒ–è¯„ä¼°å™¨ç®¡ç†å™¨"""
        # ä¸ºæ··åˆæ¨¡å‹è¯„ä¼°å™¨å‡†å¤‡ä¸¤ç§é…ç½®
        self.chat_config = chat_config.copy()
        self.embedding_config = embedding_config.copy()
        self.evaluators = {}  # å°†åœ¨åˆå§‹åŒ–æ—¶å¼‚æ­¥åˆ›å»º
        
        logger.info(f"ğŸ”§ è¯„ä¼°å™¨ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize_async(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ‰€æœ‰è¯„ä¼°å™¨"""
        # ä¸ºå¢å¼ºå­¦æœ¯è¯„ä¼°å™¨åˆå¹¶é…ç½®
        enhanced_config = {
            **self.chat_config,
            "chat_api_key": self.chat_config.get("api_key"),
            "chat_base_url": self.chat_config.get("base_url"),
            "chat_model": self.chat_config.get("model"),
            "embedding_api_key": self.embedding_config.get("api_key"),
            "embedding_base_url": self.embedding_config.get("base_url"),
            "embedding_model": self.embedding_config.get("model"),
            "evaluation_mode": "hybrid"  # ä½¿ç”¨æ··åˆæ¨¡å¼ï¼šembeddingè®¡ç®—ç›¸å…³æ€§ï¼ŒèŠå¤©æ¨¡å‹è¯„ä¼°è´¨é‡
        }
        
        self.evaluators = await EvaluatorFactory.create_all_evaluators_async(enhanced_config)
        
        if not self.evaluators:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°å™¨")
        
        logger.info(f"ğŸ”§ å¯ç”¨çš„è¯„ä¼°å™¨: {list(self.evaluators.keys())}")
    
    async def evaluate_all_async(self, questions: List[str], answers: List[str], 
                               ground_truths: List[str], contexts: List[List[str]] = None) -> Dict[str, Dict[str, List[float]]]:
        """å¼‚æ­¥æ‰§è¡Œæ‰€æœ‰è¯„ä¼°å™¨è¯„ä¼°"""
        all_results = {}
        
        for evaluator_name, evaluator in self.evaluators.items():
            logger.info(f"\nğŸ“Š ä½¿ç”¨{evaluator_name}è¯„ä¼°å™¨è¯„ä¼°ä¸­...")
            
            try:
                # ä½¿ç”¨å¸¦è¶…æ—¶çš„å¼‚æ­¥è¯„ä¼°
                metrics = await evaluator.evaluate_with_timeout(
                    questions, answers, ground_truths, contexts
                )
                all_results[evaluator_name] = metrics
                logger.debug(f"    âœ… å®Œæˆ")
            except Exception as e:
                logger.error(f"    âŒ å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼å¡«å……
                default_metrics = {metric: [None] * len(answers) 
                                 for metric in evaluator.get_supported_metrics()}
                all_results[evaluator_name] = default_metrics
        
        return all_results
    
    def get_evaluator_summary(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°å™¨æ¦‚è¦"""
        summary = {
            "total_evaluators": len(self.evaluators),
            "available_evaluators": list(self.evaluators.keys()),
            "evaluator_details": {}
        }
        
        for name, evaluator in self.evaluators.items():
            summary["evaluator_details"][name] = evaluator.get_evaluator_info()
        
        return summary